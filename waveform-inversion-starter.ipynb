{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":39763,"databundleVersionId":11756775,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/paulharrald/waveform-inversion-starter?scriptVersionId=237116474\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Complete DataLoader Performance Optimization\n\n# First, check available system resources\nimport os\nimport psutil\nimport torch\nimport multiprocessing\n\n# Get CPU and memory information\ncpu_count = os.cpu_count() or multiprocessing.cpu_count()\navailable_memory = psutil.virtual_memory().available / (1024 ** 3)  # GB\ngpu_available = torch.cuda.is_available()\n\nprint(f\"System resources:\")\nprint(f\"- CPU cores: {cpu_count}\")\nprint(f\"- Available memory: {available_memory:.2f} GB\")\nprint(f\"- GPU available: {gpu_available}\")\nif gpu_available:\n    for i in range(torch.cuda.device_count()):\n        print(f\"- GPU {i}: {torch.cuda.get_device_name(i)}\")\n        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / (1024**3):.2f} GB\")\n\n# Calculate optimal settings\noptimal_workers = min(16, max(4, cpu_count - 2))  # Don't use all cores, leave some for system\noptimal_batch_size = CONFIG['batch_size']  # Keep current batch size unless memory issues\n\n# Print recommendations\nprint(\"\\nRecommended optimizations:\")\nprint(f\"1. Increase num_workers from {CONFIG['num_workers']} to {optimal_workers}\")\nprint(f\"2. Enable pin_memory=True for faster CPU to GPU transfers\")\nprint(f\"3. Set persistent_workers=True to avoid worker initialization overhead\")\nprint(f\"4. Set prefetch_factor=2 for efficient prefetching\")\n\n# Apply optimizations to CONFIG\nCONFIG.update({\n    'num_workers': optimal_workers,\n    'pin_memory': True,         # Enable pin_memory for faster GPU transfers\n    'persistent_workers': True, # Keep workers alive between data loading calls\n    'prefetch_factor': 2        # How many batches to prefetch per worker\n})\n\nprint(\"\\nUpdated configuration:\")\nprint(f\"- num_workers: {CONFIG['num_workers']}\")\nprint(f\"- pin_memory: {CONFIG.get('pin_memory', False)}\")\nprint(f\"- persistent_workers: {CONFIG.get('persistent_workers', False)}\")\nprint(f\"- prefetch_factor: {CONFIG.get('prefetch_factor', 2)}\")\n\n# Example of how to apply these settings in your DataLoader creation (for reference)\n\"\"\"\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=CONFIG['batch_size'],\n    shuffle=True,\n    num_workers=CONFIG['num_workers'],\n    pin_memory=CONFIG.get('pin_memory', False),\n    persistent_workers=CONFIG.get('persistent_workers', False),\n    prefetch_factor=CONFIG.get('prefetch_factor', 2)\n)\n\"\"\"\n\nprint(\"\\nNote: These settings should be applied to your DataLoader creation in cell 2\")\nprint(\"Expected impact: Significantly faster data loading, reduced GPU idle time\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T22:14:14.582712Z","iopub.execute_input":"2025-04-30T22:14:14.583069Z","iopub.status.idle":"2025-04-30T22:14:14.595569Z","shell.execute_reply.started":"2025-04-30T22:14:14.583027Z","shell.execute_reply":"2025-04-30T22:14:14.594714Z"}},"outputs":[{"name":"stdout","text":"System resources:\n- CPU cores: 4\n- Available memory: 25.81 GB\n- GPU available: True\n- GPU 0: Tesla T4\n  Memory: 14.74 GB\n- GPU 1: Tesla T4\n  Memory: 14.74 GB\n\nRecommended optimizations:\n1. Increase num_workers from 2 to 4\n2. Enable pin_memory=True for faster CPU to GPU transfers\n3. Set persistent_workers=True to avoid worker initialization overhead\n4. Set prefetch_factor=2 for efficient prefetching\n\nUpdated configuration:\n- num_workers: 4\n- pin_memory: True\n- persistent_workers: True\n- prefetch_factor: 2\n\nNote: These settings should be applied to your DataLoader creation in cell 2\nExpected impact: Significantly faster data loading, reduced GPU idle time\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Cell 1: Import Libraries and Major Configuration Block\nimport os\nimport time\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Set random seeds for reproducibility\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# ==== MASTER CONFIGURATION BLOCK ====\nCONFIG = {\n    # Runtime configuration\n    'approach': 'feature_detection',  # Options: 'thresholding', 'physics_guided', 'feature_detection'\n    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    'mixed_precision': True,  # Use mixed precision training for GPU acceleration\n    'save_models': True,  # Save models during training\n    'compare_approaches': False,  # Run multiple approaches and compare\n    \n    # Data configuration - ADJUST THESE PATHS FOR YOUR ENVIRONMENT\n    'data_dir': Path(\"/kaggle/input/waveform-inversion/train_samples\"),\n    'test_dir': Path(\"/kaggle/input/waveform-inversion/test\"),\n    'output_dir': Path(\"./outputs\"),\n    \n    # Dataset parameters\n    'val_size': 0.15,  # Validation set size\n    'batch_size': 16,  # GPU-friendly batch size\n    'num_workers': 8,  # DataLoader workers\n    \n    # Training parameters\n    'num_epochs': 200,\n    'learning_rate': 1e-4,\n    'weight_decay': 1e-5,\n    'early_stopping': 10,\n    'scheduler_patience': 5,\n    'prefetch_factor': 2,     # Controls batch prefetching per worker\n    \n    # Model parameters\n    'in_channels': 5,\n    'out_channels': 1,\n    'hidden_dim': 64,  # Base dimensionality for models\n    \n    # Physics-guided parameters\n    'wave_eq_weight': 0.15,\n    'slowness_weight': 0.2,\n    'layering_weight': 0.1,\n    'contrast_weight': 0.5,\n    \n    # Feature detection parameters\n    'salt_weight': 1.0,\n    'fault_weight': 1.0,\n    'layer_weight': 1.0,\n    'geological_constraint_weight': 0.3,\n    \n    # Thresholding parameters\n    'threshold_method': 'otsu',  # Options: 'otsu', 'mean', 'adaptive'\n    'edge_enhancement': 1.5,\n    'use_morphology': True,\n    \n    # Submission parameters\n    'ensemble_submission': False,  # Use ensemble of multiple models\n    'post_process': True,  # Apply post-processing to predictions\n    'submission_path': \"submission.csv\",\n}\n\n# Create output directory\nos.makedirs(CONFIG['output_dir'], exist_ok=True)\nos.makedirs(CONFIG['output_dir'] / 'models', exist_ok=True)\nos.makedirs(CONFIG['output_dir'] / 'visualizations', exist_ok=True)\n\n# Print configuration summary\nprint(f\"Running with approach: {CONFIG['approach']}\")\nprint(f\"Device: {CONFIG['device']}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Mixed precision: {CONFIG['mixed_precision']}\")\n\n# Initialize experiment name with timestamp\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nexperiment_name = f\"{CONFIG['approach']}_{timestamp}\"\nprint(f\"Experiment name: {experiment_name}\")\n\n# Save configuration\nconfig_path = CONFIG['output_dir'] / f\"config_{experiment_name}.txt\"\nwith open(config_path, 'w') as f:\n    for key, value in CONFIG.items():\n        f.write(f\"{key}: {value}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T21:55:51.128435Z","iopub.execute_input":"2025-04-30T21:55:51.128706Z","iopub.status.idle":"2025-04-30T21:55:53.67688Z","shell.execute_reply.started":"2025-04-30T21:55:51.128669Z","shell.execute_reply":"2025-04-30T21:55:53.676171Z"}},"outputs":[{"name":"stdout","text":"Running with approach: feature_detection\nDevice: cuda\nGPU: Tesla T4\nMixed precision: True\nExperiment name: feature_detection_20250430_215553\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Cell 2: Data Loading and Dataset Classes\nclass SeismicDataset(Dataset):\n    \"\"\"Dataset class for seismic data with flexible input/output handling\"\"\"\n    def __init__(self, input_files, output_files=None, transform=None, \n                 normalize=True, gain=True, augment=False, \n                 binary_threshold=None, feature_mode=False):\n        \"\"\"\n        Args:\n            input_files: List of seismic data file paths\n            output_files: List of velocity map file paths (None for test set)\n            transform: Optional transforms to apply\n            normalize: Whether to normalize data\n            gain: Whether to apply time-dependent gain\n            augment: Whether to apply data augmentation\n            binary_threshold: If not None, convert velocity maps to binary using this threshold\n            feature_mode: If True, extract geological features from velocity maps\n        \"\"\"\n        self.input_files = input_files\n        self.output_files = output_files\n        self.transform = transform\n        self.normalize = normalize\n        self.gain = gain\n        self.augment = augment\n        self.binary_threshold = binary_threshold\n        self.feature_mode = feature_mode\n        \n        # Build index map for efficient access\n        self.index_map = []\n        for i, f in enumerate(self.input_files):\n            try:\n                data = np.load(f, mmap_mode='r')\n                for j in range(data.shape[0]):\n                    self.index_map.append((i, j))\n            except Exception as e:\n                print(f\"Error loading {f}: {e}\")\n        \n        # Calculate statistics if normalizing\n        if normalize and output_files:\n            self.calc_stats()\n    \n    def calc_stats(self, max_files=10, max_samples=100):\n        \"\"\"Calculate dataset statistics for normalization\"\"\"\n        print(\"Calculating dataset statistics...\")\n        # Input statistics\n        in_samples = []\n        for i in range(min(max_files, len(self.input_files))):\n            try:\n                data = np.load(self.input_files[i], mmap_mode='r')\n                idx = np.random.choice(data.shape[0], min(max_samples, data.shape[0]), replace=False)\n                in_samples.append(data[idx])\n            except Exception as e:\n                print(f\"Error in stats calculation for {self.input_files[i]}: {e}\")\n        \n        if in_samples:\n            in_array = np.concatenate(in_samples, axis=0)\n            self.in_mean = float(np.mean(in_array))\n            self.in_std = float(np.std(in_array))\n        else:\n            self.in_mean, self.in_std = 0.0, 1.0\n            \n        # Output statistics (if available)\n        if self.output_files:\n            out_samples = []\n            for i in range(min(max_files, len(self.output_files))):\n                try:\n                    data = np.load(self.output_files[i], mmap_mode='r')\n                    idx = np.random.choice(data.shape[0], min(max_samples, data.shape[0]), replace=False)\n                    \n                    # Handle different output shapes\n                    if len(data.shape) == 4:  # [batch, channel, height, width]\n                        out_samples.append(data[idx])\n                    elif len(data.shape) == 3:  # [batch, height, width]\n                        out_samples.append(data[idx, np.newaxis])\n                except Exception as e:\n                    print(f\"Error in stats calculation for {self.output_files[i]}: {e}\")\n            \n            if out_samples:\n                out_array = np.concatenate(out_samples, axis=0)\n                self.out_mean = float(np.mean(out_array))\n                self.out_std = float(np.std(out_array))\n                \n                # For binary threshold detection\n                if self.binary_threshold is None:\n                    try:\n                        from skimage.filters import threshold_otsu\n                        self.auto_threshold = threshold_otsu(out_array.flatten())\n                    except:\n                        self.auto_threshold = self.out_mean\n                else:\n                    self.auto_threshold = self.binary_threshold\n            else:\n                self.out_mean, self.out_std = 0.0, 1.0\n                self.auto_threshold = 0.5\n                \n        print(f\"Input stats: mean={self.in_mean:.4f}, std={self.in_std:.4f}\")\n        if self.output_files:\n            print(f\"Output stats: mean={self.out_mean:.4f}, std={self.out_std:.4f}\")\n            print(f\"Auto threshold: {self.auto_threshold:.4f}\")\n    \n    def __len__(self):\n        return len(self.index_map)\n    \n    def apply_gain(self, x):\n        \"\"\"Apply time-dependent gain to seismic data\"\"\"\n        # Time-dependent gain increases amplitude with time\n        time_steps = x.shape[1]\n        time = np.linspace(0, 1, time_steps)\n        gain = (time ** 2)[:, np.newaxis]  # Square gain with time\n        \n        # Apply to each channel\n        gained_x = x.copy()\n        for c in range(x.shape[0]):\n            gained_x[c] = x[c] * gain\n            \n        return gained_x\n    \n    def extract_geological_features(self, y):\n        \"\"\"Extract geological features from velocity map\"\"\"\n        # 1. Salt body detection (high velocity regions)\n        salt_mask = (y > self.auto_threshold + 0.2 * self.out_std).astype(np.float32)\n        \n        # 2. Fault detection (using edge detection)\n        from scipy import ndimage\n        sobel_x = ndimage.sobel(y, axis=1)\n        sobel_y = ndimage.sobel(y, axis=0)\n        grad_mag = np.sqrt(sobel_x**2 + sobel_y**2)\n        grad_mag = grad_mag / (np.max(grad_mag) + 1e-8)\n        fault_mask = (grad_mag > 0.3).astype(np.float32)\n        \n        # 3. Layer detection (horizontal structures)\n        # Use horizontal gradient\n        layer_edges = np.abs(sobel_y)\n        layer_edges = layer_edges / (np.max(layer_edges) + 1e-8)\n        layer_mask = (layer_edges > 0.2).astype(np.float32)\n        \n        # Stack into feature channels [salt, fault, layer]\n        features = np.stack([salt_mask, fault_mask, layer_mask], axis=0)\n        return features\n    \n    def apply_augmentation(self, x, y=None):\n        \"\"\"Apply data augmentation\"\"\"\n        # Flip horizontally with 50% probability\n        if np.random.random() > 0.5:\n            x = np.flip(x, axis=2).copy()  # Flip along receiver dimension\n            if y is not None:\n                y = np.flip(y, axis=-1).copy()  # Flip along width\n        \n        # Add small random noise with 30% probability\n        if np.random.random() > 0.7:\n            noise_level = np.random.uniform(0, 0.05)\n            x = x + np.random.normal(0, noise_level, size=x.shape)\n        \n        return x, y\n        \n    def __getitem__(self, idx):\n        \"\"\"Get a single sample from the dataset\"\"\"\n        file_idx, sample_idx = self.index_map[idx]\n        \n        # Load input data\n        x = np.load(self.input_files[file_idx], mmap_mode='r')[sample_idx].astype(np.float32)\n        \n        # Apply gain if requested\n        if self.gain:\n            x = self.apply_gain(x)\n            \n        # Load output data if available (for training)\n        if self.output_files:\n            y_data = np.load(self.output_files[file_idx], mmap_mode='r')[sample_idx]\n            \n            # Handle different output shapes\n            if len(y_data.shape) == 2:  # (height, width)\n                y = y_data\n            elif len(y_data.shape) == 3 and y_data.shape[0] == 1:  # (1, height, width)\n                y = y_data[0]\n            elif len(y_data.shape) > 2:  # Multi-dimensional\n                y = y_data.reshape(-1, y_data.shape[-2], y_data.shape[-1])[0]\n            else:\n                raise ValueError(f\"Unexpected velocity map shape: {y_data.shape}\")\n                \n            y = y.astype(np.float32)\n            \n            # Convert to binary if threshold is provided\n            if self.binary_threshold is not None:\n                threshold = self.binary_threshold if self.binary_threshold > 0 else self.auto_threshold\n                y = (y > threshold).astype(np.float32)\n                \n            # Extract geological features if in feature mode\n            if self.feature_mode:\n                y = self.extract_geological_features(y)\n        else:\n            # For test set, create a dummy y\n            y = np.zeros((1, x.shape[-1], x.shape[-1]), dtype=np.float32)\n            \n        # Apply data augmentation if enabled\n        if self.augment:\n            x, y = self.apply_augmentation(x, y)\n            \n        # Normalize if enabled\n        if self.normalize:\n            x = (x - self.in_mean) / (self.in_std + 1e-6)\n            if self.output_files and not self.feature_mode and self.binary_threshold is None:\n                y = (y - self.out_mean) / (self.out_std + 1e-6)\n                \n        # Convert to PyTorch tensors\n        x_tensor = torch.from_numpy(x).float()\n        \n        # Ensure y has proper dimensions for loss functions\n        if not self.feature_mode and len(y.shape) == 2:\n            y = y[np.newaxis, ...]  # Add channel dimension if needed\n        y_tensor = torch.from_numpy(y).float()\n        \n        # Apply transforms if provided\n        if self.transform:\n            x_tensor, y_tensor = self.transform(x_tensor, y_tensor)\n            \n        return x_tensor, y_tensor\n\n\ndef load_and_prepare_data(config):\n    \"\"\"Prepare datasets and dataloaders based on configuration\"\"\"\n    data_dir = config['data_dir']\n    \n    # Find input and output files\n    print(\"Loading competition dataset...\")\n    input_files = sorted([f for f in data_dir.rglob(\"*.npy\") \n                         if 'seis' in f.name or 'data' in f.name])\n    output_files = [Path(str(f).replace(\"seis\", \"vel\").replace(\"data\", \"model\")) \n                    for f in input_files]\n    \n    print(f\"Found {len(input_files)} input files and {len(output_files)} output files\")\n    \n    # Verify files exist\n    input_files = [f for f in input_files if f.exists()]\n    output_files = [f for f in output_files if f.exists()]\n    if len(input_files) != len(output_files):\n        print(f\"Warning: Mismatched file counts - {len(input_files)} inputs, {len(output_files)} outputs\")\n        # Keep only matching pairs\n        input_basenames = [f.stem for f in input_files]\n        output_basenames = [f.stem.replace(\"vel\", \"seis\").replace(\"model\", \"data\") for f in output_files]\n        common_basenames = set(input_basenames) & set(output_basenames)\n        input_files = [f for f in input_files if f.stem in common_basenames]\n        output_files = [f for f in output_files if f.stem.replace(\"vel\", \"seis\").replace(\"model\", \"data\") in common_basenames]\n        print(f\"Kept {len(input_files)} matching file pairs\")\n    \n    # Split into train and validation sets\n    train_in, val_in, train_out, val_out = train_test_split(\n        input_files, output_files, test_size=config['val_size'], random_state=SEED\n    )\n    \n    print(f\"Training set: {len(train_in)} files\")\n    print(f\"Validation set: {len(val_in)} files\")\n    \n    # Determine binary threshold and feature mode based on approach\n    binary_threshold = None\n    feature_mode = False\n    \n    if config['approach'] == 'thresholding':\n        binary_threshold = 0  # Use auto-detected threshold\n    elif config['approach'] == 'feature_detection':\n        feature_mode = True\n        \n    # Create datasets\n    train_ds = SeismicDataset(\n        train_in, train_out,\n        normalize=True,\n        gain=True,\n        augment=True,\n        binary_threshold=binary_threshold,\n        feature_mode=feature_mode\n    )\n    \n    val_ds = SeismicDataset(\n        val_in, val_out,\n        normalize=True,\n        gain=True,\n        augment=False,\n        binary_threshold=binary_threshold,\n        feature_mode=feature_mode\n    )\n    \n# Optimized DataLoader creation\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        num_workers=config['num_workers'],\n        pin_memory=torch.cuda.is_available(),\n        persistent_workers=config['num_workers'] > 0,\n        prefetch_factor=config.get('prefetch_factor', 2)\n    )\n    \n    val_loader = DataLoader(\n        val_ds,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers'],\n        pin_memory=torch.cuda.is_available(),\n        persistent_workers=config['num_workers'] > 0,\n        prefetch_factor=config.get('prefetch_factor', 2)\n    )\n                \n    # Get dataset statistics for normalization and thresholding\n    stats = {\n        'input_mean': train_ds.in_mean,\n        'input_std': train_ds.in_std,\n        'output_mean': train_ds.out_mean if hasattr(train_ds, 'out_mean') else 0.0,\n        'output_std': train_ds.out_std if hasattr(train_ds, 'out_std') else 1.0,\n        'auto_threshold': train_ds.auto_threshold if hasattr(train_ds, 'auto_threshold') else 0.5\n    }\n    \n    return train_loader, val_loader, stats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T21:55:53.678817Z","iopub.execute_input":"2025-04-30T21:55:53.679162Z","iopub.status.idle":"2025-04-30T21:55:53.706918Z","shell.execute_reply.started":"2025-04-30T21:55:53.679144Z","shell.execute_reply":"2025-04-30T21:55:53.706341Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Cell 3: Model Architectures\n# 1. Physics-Guided Model for Velocity Prediction\nclass PhysicsGuidedUNet(nn.Module):\n    \"\"\"U-Net with physics-guided components for velocity prediction\"\"\"\n    def __init__(self, in_channels=5, out_channels=1, hidden_dim=64):\n        super().__init__()\n        \n        # Encoder blocks\n        self.enc1 = self._make_encoder_block(in_channels, hidden_dim)\n        self.enc2 = self._make_encoder_block(hidden_dim, hidden_dim*2)\n        self.enc3 = self._make_encoder_block(hidden_dim*2, hidden_dim*4)\n        self.enc4 = self._make_encoder_block(hidden_dim*4, hidden_dim*8)\n        \n        # Bottleneck\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(hidden_dim*8, hidden_dim*16, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim*16),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(hidden_dim*16, hidden_dim*16, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim*16),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n        \n        # Decoder blocks with skip connections\n        self.dec4 = self._make_decoder_block(hidden_dim*16 + hidden_dim*8, hidden_dim*8)\n        self.dec3 = self._make_decoder_block(hidden_dim*8 + hidden_dim*4, hidden_dim*4)\n        self.dec2 = self._make_decoder_block(hidden_dim*4 + hidden_dim*2, hidden_dim*2)\n        self.dec1 = self._make_decoder_block(hidden_dim*2 + hidden_dim, hidden_dim)\n        \n        # Final output layer\n        self.final = nn.Sequential(\n            nn.Conv2d(hidden_dim, out_channels, kernel_size=1),\n            nn.Sigmoid()  # Output in [0,1] range\n        )\n        \n        # Pooling layer for downsampling\n        self.pool = nn.MaxPool2d(2)\n        \n        # Physics-guided layers\n        # Edge detection for geological boundaries\n        self.edge_detector = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.edge_detector.weight.data = torch.tensor([\n            [[-1, -1, -1],\n             [2, 2, 2],\n             [-1, -1, -1]]\n        ], dtype=torch.float32).view(1, 1, 3, 3).repeat(out_channels, out_channels, 1, 1)\n        self.edge_detector.weight.requires_grad = False\n        \n    def _make_encoder_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n    \n    def _make_decoder_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n    \n    def enhance_contrast(self, x):\n        \"\"\"Enhance contrast in the prediction to match high-contrast ground truth\"\"\"\n        # Apply tanh with scaling to enhance contrast\n        enhanced = torch.tanh(5 * (x - 0.5)) * 0.5 + 0.5\n        return enhanced\n    \n    def enhance_edges(self, x):\n        \"\"\"Enhance geological edges in the prediction\"\"\"\n        edge_features = self.edge_detector(x)\n        enhanced = x + 0.1 * edge_features\n        return torch.clamp(enhanced, 0, 1)\n    \n    def forward(self, x):\n        # Input format adjustment - seismic data comes as [B, C, T, R]\n        # We'll permute to [B, C, R, T] for 2D convolutions\n        x = x.permute(0, 1, 3, 2)\n        \n        # Encoder path with skip connections\n        e1 = self.enc1(x)\n        p1 = self.pool(e1)\n        \n        e2 = self.enc2(p1)\n        p2 = self.pool(e2)\n        \n        e3 = self.enc3(p2)\n        p3 = self.pool(e3)\n        \n        e4 = self.enc4(p3)\n        p4 = self.pool(e4)\n        \n        # Bottleneck\n        b = self.bottleneck(p4)\n        \n        # Decoder path with skip connections\n        d4 = F.interpolate(b, size=e4.shape[2:], mode='bilinear', align_corners=False)\n        d4 = self.dec4(torch.cat([d4, e4], dim=1))\n        \n        d3 = F.interpolate(d4, size=e3.shape[2:], mode='bilinear', align_corners=False)\n        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n        \n        d2 = F.interpolate(d3, size=e2.shape[2:], mode='bilinear', align_corners=False)\n        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n        \n        d1 = F.interpolate(d2, size=e1.shape[2:], mode='bilinear', align_corners=False)\n        d1 = self.dec1(torch.cat([d1, e1], dim=1))\n        \n        # Final output and enhancement\n        out = self.final(d1)\n        out = self.enhance_contrast(out)\n        out = self.enhance_edges(out)\n        \n        # Ensure output is 70x70 as expected\n        out = F.interpolate(out, size=(70, 70), mode='bilinear', align_corners=False)\n        \n        return out\n\n\n# 2. Geological Feature Detection Model\nclass GeologicalFeatureNet(nn.Module):\n    \"\"\"Neural network for detecting geological features from seismic data\"\"\"\n    def __init__(self, in_channels=5, hidden_dim=64):\n        super().__init__()\n        \n        # Encoder (shared for all features)\n        self.encoder = nn.Sequential(\n            nn.Conv2d(in_channels, hidden_dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            \n            nn.Conv2d(hidden_dim, hidden_dim*2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim*2),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            \n            nn.Conv2d(hidden_dim*2, hidden_dim*4, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim*4),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            \n            nn.Conv2d(hidden_dim*4, hidden_dim*8, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim*8),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        \n        # Specialized decoders for different geological features\n        \n        # 1. Salt body detector - MODIFIED to remove sigmoid for mixed precision compatibility\n        self.salt_decoder = nn.Sequential(\n            nn.Conv2d(hidden_dim*8, hidden_dim*4, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim*4),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            nn.Conv2d(hidden_dim*4, hidden_dim*2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim*2),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            nn.Conv2d(hidden_dim*2, hidden_dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            nn.Conv2d(hidden_dim, hidden_dim//2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim//2),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            nn.Conv2d(hidden_dim//2, 1, kernel_size=1)\n            # Sigmoid removed for BCEWithLogitsLoss compatibility\n        )\n        \n        # 2. Fault detector with edge awareness - MODIFIED to remove sigmoid\n        self.fault_decoder = nn.Sequential(\n            nn.Conv2d(hidden_dim*8, hidden_dim*4, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim*4),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            nn.Conv2d(hidden_dim*4, hidden_dim*2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim*2),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            nn.Conv2d(hidden_dim*2, hidden_dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            nn.Conv2d(hidden_dim, hidden_dim//2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim//2),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            nn.Conv2d(hidden_dim//2, 1, kernel_size=1)\n            # Sigmoid removed for BCEWithLogitsLoss compatibility\n        )\n        \n        # 3. Layer boundary detector with horizontal bias - MODIFIED to remove sigmoid\n        self.layer_decoder = nn.Sequential(\n            nn.Conv2d(hidden_dim*8, hidden_dim*4, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim*4),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            nn.Conv2d(hidden_dim*4, hidden_dim*2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim*2),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            nn.Conv2d(hidden_dim*2, hidden_dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            nn.Conv2d(hidden_dim, hidden_dim//2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim//2),\n            nn.ReLU(),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            nn.Conv2d(hidden_dim//2, 1, kernel_size=1)\n            # Sigmoid removed for BCEWithLogitsLoss compatibility\n        )\n        \n        # Optional velocity reconstruction from geological features - Keep sigmoid here\n        self.velocity_reconstruction = nn.Sequential(\n            nn.Conv2d(3, hidden_dim//2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim//2),\n            nn.ReLU(),\n            \n            nn.Conv2d(hidden_dim//2, hidden_dim//4, kernel_size=3, padding=1),\n            nn.BatchNorm2d(hidden_dim//4),\n            nn.ReLU(),\n            \n            nn.Conv2d(hidden_dim//4, 1, kernel_size=1),\n            nn.Sigmoid()  # Keep sigmoid here as this is not used with BCE loss\n        )\n    \n    def forward(self, x):\n        # Input format adjustment\n        x = x.permute(0, 1, 3, 2)  # [B, C, T, R] to [B, C, R, T]\n        \n        # Encode features\n        encoded = self.encoder(x)\n        \n        # Decode for each geological feature\n        salt_mask = self.salt_decoder(encoded)\n        fault_lines = self.fault_decoder(encoded)\n        layer_boundaries = self.layer_decoder(encoded)\n        \n        # Ensure all outputs are 70x70\n        salt_mask = F.interpolate(salt_mask, size=(70, 70), mode='bilinear', align_corners=False)\n        fault_lines = F.interpolate(fault_lines, size=(70, 70), mode='bilinear', align_corners=False)\n        layer_boundaries = F.interpolate(layer_boundaries, size=(70, 70), mode='bilinear', align_corners=False)\n        \n        # Apply sigmoid for the velocity reconstruction input\n        salt_prob = torch.sigmoid(salt_mask)\n        fault_prob = torch.sigmoid(fault_lines)\n        layer_prob = torch.sigmoid(layer_boundaries)\n        \n        # Combine features for velocity reconstruction \n        features = torch.cat([salt_prob, fault_prob, layer_prob], dim=1)\n        velocity = self.velocity_reconstruction(features)\n        \n        return {\n            'salt': salt_mask,         # Now returns logits\n            'faults': fault_lines,     # Now returns logits\n            'layers': layer_boundaries, # Now returns logits\n            'velocity': velocity       # Still returns probabilities\n        }\n\n\ndef create_model(config):\n    \"\"\"Create model based on configuration\"\"\"\n    approach = config['approach']\n    device = config['device']\n    \n    if approach == 'physics_guided':\n        model = PhysicsGuidedUNet(\n            in_channels=config['in_channels'],\n            out_channels=config['out_channels'],\n            hidden_dim=config['hidden_dim']\n        )\n        print(\"Created Physics-Guided U-Net model\")\n    elif approach == 'feature_detection':\n        model = GeologicalFeatureNet(\n            in_channels=config['in_channels'],\n            hidden_dim=config['hidden_dim']\n        )\n        print(\"Created Geological Feature Detection model\")\n    else:\n        # Default to physics-guided model\n        model = PhysicsGuidedUNet(\n            in_channels=config['in_channels'],\n            out_channels=config['out_channels'],\n            hidden_dim=config['hidden_dim']\n        )\n        print(f\"Unknown approach '{approach}', defaulting to Physics-Guided model\")\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # Print model summary\n    num_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Model has {num_params:,} parameters ({trainable_params:,} trainable)\")\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T21:55:53.707897Z","iopub.execute_input":"2025-04-30T21:55:53.708152Z","iopub.status.idle":"2025-04-30T21:55:53.73847Z","shell.execute_reply.started":"2025-04-30T21:55:53.708135Z","shell.execute_reply":"2025-04-30T21:55:53.737752Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Cell 4: Loss Functions (Fixed for ALL Mixed Precision Issues)\n\nclass PhysicsGuidedLoss(nn.Module):\n    \"\"\"Loss function for physics-guided velocity prediction\"\"\"\n    def __init__(self, wave_eq_weight=0.15, slowness_weight=0.2, \n                 layering_weight=0.1, contrast_weight=0.5):\n        super().__init__()\n        self.wave_eq_weight = wave_eq_weight\n        self.slowness_weight = slowness_weight\n        self.layering_weight = layering_weight\n        self.contrast_weight = contrast_weight\n        \n        # Edge detection kernels\n        self.sobel_x = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=torch.float32).view(1, 1, 3, 3)\n        self.sobel_y = torch.tensor([[1,2,1],[0,0,0],[-1,-2,-1]], dtype=torch.float32).view(1, 1, 3, 3)\n    \n    def _prepare_kernels(self, device):\n        \"\"\"Move kernels to the correct device\"\"\"\n        if self.sobel_x.device != device:\n            self.sobel_x = self.sobel_x.to(device)\n            self.sobel_y = self.sobel_y.to(device)\n    \n    def data_fidelity_loss(self, pred, target):\n        \"\"\"Standard L1 loss for overall accuracy\"\"\"\n        return F.l1_loss(pred, target)\n    \n    def wave_equation_constraint(self, pred):\n        \"\"\"Simplified wave equation physics constraint\"\"\"\n        device = pred.device\n        self._prepare_kernels(device)\n        \n        # Second-order spatial derivatives (Laplacian)\n        laplacian_x = F.conv2d(\n            F.pad(pred, (1, 1, 0, 0), mode='replicate'),\n            torch.tensor([[[[1, -2, 1]]]], device=device),\n            padding=0\n        )\n        \n        laplacian_y = F.conv2d(\n            F.pad(pred, (0, 0, 1, 1), mode='replicate'),\n            torch.tensor([[[[1], [-2], [1]]]], device=device),\n            padding=0\n        )\n        \n        laplacian = laplacian_x + laplacian_y\n        \n        # Penalize non-physical velocity distributions\n        # Wave equation: ∇²u = (1/v²) * ∂²u/∂t² (we're penalizing deviations from Laplacian behavior)\n        return torch.mean(torch.abs(laplacian))\n    \n    def slowness_gradient_constraint(self, pred):\n        \"\"\"Constraint to encourage smoothness in slowness (1/v)\"\"\"\n        device = pred.device\n        self._prepare_kernels(device)\n        \n        # Convert velocity to slowness (1/v)\n        eps = 1e-6\n        slowness = 1.0 / (pred + eps)\n        \n        # Calculate gradients of slowness\n        grad_x = F.conv2d(slowness, self.sobel_x, padding=1)\n        grad_y = F.conv2d(slowness, self.sobel_y, padding=1)\n        \n        # Penalize large gradients in slowness (encourage smoothness)\n        return torch.mean(torch.abs(grad_x)) + torch.mean(torch.abs(grad_y))\n    \n    def geological_layering_constraint(self, pred):\n        \"\"\"Encourage horizontal layering typical in geological structures\"\"\"\n        device = pred.device\n        self._prepare_kernels(device)\n        \n        # Calculate horizontal and vertical gradients\n        grad_x = F.conv2d(pred, self.sobel_x, padding=1)\n        grad_y = F.conv2d(pred, self.sobel_y, padding=1)\n        \n        # Ratio of vertical to horizontal gradients\n        # Large values where vertical gradients are much larger than horizontal (layering)\n        ratio = torch.abs(grad_y) / (torch.abs(grad_x) + 1e-6)\n        \n        # Penalize small ratios (encourage layering)\n        return torch.mean(torch.exp(-ratio))\n    \n    def contrast_enhancement_loss(self, pred, target):\n        \"\"\"Loss to encourage high contrast similar to ground truth\"\"\"\n        # Calculate histograms\n        pred_hist = torch.histc(pred, bins=10, min=0, max=1)\n        target_hist = torch.histc(target, bins=10, min=0, max=1)\n        \n        # Normalize histograms\n        pred_hist = pred_hist / (pred_hist.sum() + 1e-6)\n        target_hist = target_hist / (target_hist.sum() + 1e-6)\n        \n        # Calculate difference between histograms (EMD approximation)\n        hist_diff = F.l1_loss(\n            torch.cumsum(pred_hist, dim=0),\n            torch.cumsum(target_hist, dim=0)\n        )\n        \n        # Penalize low contrast (encourage bi-modal distribution)\n        contrast = torch.var(pred)\n        contrast_target = torch.var(target)\n        \n        return hist_diff + torch.abs(contrast - contrast_target)\n    \n    def forward(self, pred, target):\n        # Data fidelity (supervised loss)\n        l1_loss = self.data_fidelity_loss(pred, target)\n        \n        # Physics-guided constraints\n        wave_eq_loss = self.wave_equation_constraint(pred)\n        slowness_loss = self.slowness_gradient_constraint(pred)\n        layering_loss = self.geological_layering_constraint(pred)\n        contrast_loss = self.contrast_enhancement_loss(pred, target)\n        \n        # Combine losses with weights\n        total_loss = (\n            l1_loss +\n            self.wave_eq_weight * wave_eq_loss +\n            self.slowness_weight * slowness_loss +\n            self.layering_weight * layering_loss +\n            self.contrast_weight * contrast_loss\n        )\n        \n        return total_loss\n\n\nclass DiceLoss(nn.Module):\n    \"\"\"Dice loss for better boundary detection - FIXED for tensor compatibility\"\"\"\n    def __init__(self, smooth=1e-6):\n        super().__init__()\n        self.smooth = smooth\n        \n    def forward(self, pred, target):\n        # Ensure tensors have the same shape and use reshape instead of view\n        # This fixes the \"view size is not compatible\" error\n        pred_flat = pred.reshape(-1)\n        target_flat = target.reshape(-1)\n        \n        # Calculate Dice coefficient\n        intersection = torch.sum(pred_flat * target_flat)\n        pred_sum = torch.sum(pred_flat * pred_flat)\n        target_sum = torch.sum(target_flat * target_flat)\n        \n        dice = (2.0 * intersection + self.smooth) / (pred_sum + target_sum + self.smooth)\n        \n        return 1.0 - dice\n\n\nclass GeologicalFeatureLoss(nn.Module):\n    \"\"\"Loss function for geological feature detection - FIXED for mixed precision\"\"\"\n    def __init__(self, salt_weight=1.0, fault_weight=1.0, layer_weight=1.0, \n                 constraint_weight=0.3):\n        super().__init__()\n        self.salt_weight = salt_weight\n        self.fault_weight = fault_weight\n        self.layer_weight = layer_weight\n        self.constraint_weight = constraint_weight\n        \n        # Base loss functions - FIXED: using BCEWithLogitsLoss for mixed precision compatibility\n        self.bce_loss = nn.BCEWithLogitsLoss()\n        self.dice_loss = DiceLoss()\n    \n    def forward(self, predictions, targets):\n        \"\"\"\n        Args:\n            predictions: dict with keys 'salt', 'faults', 'layers', 'velocity'\n            targets: tensor of shape [B, 3, H, W] with channels [salt, faults, layers]\n        \"\"\"\n        # Separate target channels\n        salt_target = targets[:, 0:1]\n        fault_target = targets[:, 1:2]\n        layer_target = targets[:, 2:3]\n        \n        # Feature detection losses (combine BCE and Dice for better boundary detection)\n        # Apply sigmoid for Dice loss since we're now using BCEWithLogitsLoss\n        salt_loss = self.bce_loss(predictions['salt'], salt_target) + \\\n                   self.dice_loss(torch.sigmoid(predictions['salt']), salt_target)\n        \n        fault_loss = self.bce_loss(predictions['faults'], fault_target) + \\\n                    self.dice_loss(torch.sigmoid(predictions['faults']), fault_target)\n        \n        layer_loss = self.bce_loss(predictions['layers'], layer_target) + \\\n                    self.dice_loss(torch.sigmoid(predictions['layers']), layer_target)\n        \n        # Apply sigmoid to convert logits to probabilities for constraint functions\n        salt_prob = torch.sigmoid(predictions['salt'])\n        fault_prob = torch.sigmoid(predictions['faults'])\n        layer_prob = torch.sigmoid(predictions['layers'])\n        \n        # Geological constraints\n        # 1. Salt bodies should be continuous (salt body coherence)\n        salt_coherence = self.continuity_constraint(salt_prob)\n        \n        # 2. Faults should be thin, continuous lines\n        fault_thin = self.thinness_constraint(fault_prob)\n        \n        # 3. Layers should be predominantly horizontal\n        layer_horizontal = self.horizontal_bias_constraint(layer_prob)\n        \n        # 4. Geological compatibility between features\n        compatibility = self.feature_compatibility_constraint(\n            salt_prob, fault_prob, layer_prob\n        )\n        \n        # Combine all losses with weights\n        total_loss = (\n            self.salt_weight * salt_loss +\n            self.fault_weight * fault_loss +\n            self.layer_weight * layer_loss +\n            self.constraint_weight * (\n                salt_coherence + \n                fault_thin + \n                layer_horizontal + \n                compatibility\n            )\n        )\n        \n        return total_loss\n    \n    def continuity_constraint(self, pred):\n        \"\"\"Penalize fragmented structures\"\"\"\n        # Calculate gradient magnitude\n        grad_x = torch.abs(pred[:, :, :, 1:] - pred[:, :, :, :-1])\n        grad_y = torch.abs(pred[:, :, 1:, :] - pred[:, :, :-1, :])\n        \n        # Only penalize gradients within the predicted region\n        mask = (pred > 0.5).float()\n        masked_grad_x = grad_x * mask[:, :, :, :-1]\n        masked_grad_y = grad_y * mask[:, :, :-1, :]\n        \n        return torch.mean(masked_grad_x) + torch.mean(masked_grad_y)\n    \n    def thinness_constraint(self, pred):\n        \"\"\"Encourage thin fault lines\"\"\"\n        # Dilated prediction\n        dilated = F.max_pool2d(pred, kernel_size=3, stride=1, padding=1)\n        \n        # Eroded prediction\n        kernel = torch.ones(1, 1, 3, 3).to(pred.device)\n        eroded = 1.0 - F.max_pool2d(1.0 - pred, kernel_size=3, stride=1, padding=1)\n        \n        # Difference between dilated and eroded should be large for thin structures\n        thinness = torch.mean(dilated - eroded)\n        \n        return -thinness  # Negative to encourage thinness\n    \n    def horizontal_bias_constraint(self, pred):\n        \"\"\"Encourage predominantly horizontal layers\"\"\"\n        # Vertical vs horizontal gradient ratio\n        grad_x = torch.abs(pred[:, :, :, 1:] - pred[:, :, :, :-1])\n        grad_y = torch.abs(pred[:, :, 1:, :] - pred[:, :, :-1, :])\n        \n        # Mean gradients\n        mean_grad_x = torch.mean(grad_x)\n        mean_grad_y = torch.mean(grad_y)\n        \n        # Horizontal layers have larger vertical gradients\n        return mean_grad_x - mean_grad_y  # Minimize for horizontal bias\n    \n    def feature_compatibility_constraint(self, salt, faults, layers):\n        \"\"\"Enforce compatibility between different geological features - FIXED for mixed precision\"\"\"\n        # 1. Salt bodies should have clear boundaries that align with layer boundaries\n        salt_boundary = torch.abs(\n            F.avg_pool2d(salt, kernel_size=3, stride=1, padding=1) - salt\n        )\n        \n        # This eliminates the autocast error while maintaining the same functionality\n        boundary_alignment = F.mse_loss(salt_boundary, layers)\n        \n        # Alternative approach using BCE:\n        # with torch.cuda.amp.autocast(enabled=False):\n        #     boundary_alignment = F.binary_cross_entropy(salt_boundary, layers)\n        \n        # 2. Faults should often terminate layers\n        fault_layer_interaction = torch.mean(faults * layers)\n        \n        # 3. Faults rarely cut through salt bodies\n        fault_salt_exclusion = torch.mean(faults * salt)\n        \n        return boundary_alignment + fault_layer_interaction - fault_salt_exclusion\n\n\ndef create_loss_function(config):\n    \"\"\"Create loss function based on configuration\"\"\"\n    approach = config['approach']\n    device = config['device']\n    \n    if approach == 'physics_guided':\n        criterion = PhysicsGuidedLoss(\n            wave_eq_weight=config['wave_eq_weight'],\n            slowness_weight=config['slowness_weight'],\n            layering_weight=config['layering_weight'],\n            contrast_weight=config['contrast_weight']\n        )\n        print(\"Created Physics-Guided Loss\")\n    elif approach == 'feature_detection':\n        criterion = GeologicalFeatureLoss(\n            salt_weight=config['salt_weight'],\n            fault_weight=config['fault_weight'],\n            layer_weight=config['layer_weight'],\n            constraint_weight=config['geological_constraint_weight']\n        )\n        print(\"Created Geological Feature Loss\")\n    else:\n        # Default to L1 loss\n        criterion = nn.L1Loss()\n        print(f\"Unknown approach '{approach}', defaulting to L1 Loss\")\n    \n    return criterion.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T21:55:53.739204Z","iopub.execute_input":"2025-04-30T21:55:53.739432Z","iopub.status.idle":"2025-04-30T21:55:53.872809Z","shell.execute_reply.started":"2025-04-30T21:55:53.739415Z","shell.execute_reply":"2025-04-30T21:55:53.872207Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Cell 5: Training and Evaluation Functions with Additional Fixes\n\ndef train_model(model, train_loader, val_loader, criterion, config):\n    \"\"\"Train model with validation and early stopping - FIXED for PyTorch 2.0+ compatibility\"\"\"\n    device = config['device']\n    num_epochs = config['num_epochs']\n    learning_rate = config['learning_rate']\n    weight_decay = config['weight_decay']\n    early_stopping = config['early_stopping']\n    output_dir = config['output_dir']\n    \n    # Create experiment subdirectory\n    experiment_path = output_dir / 'models' / experiment_name\n    os.makedirs(experiment_path, exist_ok=True)\n    \n    # Initialize optimizer\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=learning_rate,\n        weight_decay=weight_decay\n    )\n    \n    # Learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min', \n        factor=0.5, \n        patience=config['scheduler_patience']\n    )\n    \n    # Mixed precision training (updated for PyTorch 2.0+)\n    if config['mixed_precision'] and torch.cuda.is_available():\n        scaler = torch.amp.GradScaler('cuda')\n    else:\n        scaler = None\n    \n    # Training metrics\n    best_val_loss = float('inf')\n    early_stop_counter = 0\n    train_losses = []\n    val_losses = []\n    \n    # Start training\n    print(f\"Starting training for {num_epochs} epochs...\")\n    start_time = time.time()\n    \n    for epoch in range(1, num_epochs + 1):\n        epoch_start = time.time()\n        \n        # Training phase\n        model.train()\n        epoch_train_loss = 0\n        \n        # Progress tracking\n        progress = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\")\n        \n        for batch_idx, (x, y) in enumerate(progress):\n            x, y = x.to(device), y.to(device)\n            \n            # Mixed precision forward pass\n            if scaler:\n                with torch.amp.autocast('cuda'):\n                    if config['approach'] == 'feature_detection':\n                        pred = model(x)\n                        loss = criterion(pred, y)\n                    else:\n                        pred = model(x)\n                        loss = criterion(pred, y)\n                \n                # Mixed precision backward pass\n                optimizer.zero_grad()\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                # Standard forward pass\n                if config['approach'] == 'feature_detection':\n                    pred = model(x)\n                    loss = criterion(pred, y)\n                else:\n                    pred = model(x)\n                    loss = criterion(pred, y)\n                \n                # Standard backward pass\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            \n            # Update metrics\n            epoch_train_loss += loss.item()\n            progress.set_postfix({'loss': loss.item()})\n        \n        # Calculate average training loss\n        avg_train_loss = epoch_train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        \n        # Validation phase\n        model.eval()\n        epoch_val_loss = 0\n        \n        # Additional metrics for physics-guided model\n        l1_error = 0\n        ssim_values = 0\n        \n        progress = tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\")\n        \n        with torch.no_grad():\n            for x, y in progress:\n                x, y = x.to(device), y.to(device)\n                \n                # Forward pass\n                if config['approach'] == 'feature_detection':\n                    pred = model(x)\n                    loss = criterion(pred, y)\n                    \n                    # Use reconstructed velocity for metrics\n                    pred_velocity = pred['velocity']\n                else:\n                    pred = model(x)\n                    loss = criterion(pred, y)\n                    pred_velocity = pred\n                \n                # Update metrics\n                epoch_val_loss += loss.item()\n                \n                # Calculate additional metrics\n                l1_error += F.l1_loss(pred_velocity, y[:, 0:1] if y.shape[1] > 1 else y).item()\n                \n                # Normalized for SSIM calculation\n                pred_norm = (pred_velocity - pred_velocity.min()) / (pred_velocity.max() - pred_velocity.min() + 1e-8)\n                target_norm = (y[:, 0:1] if y.shape[1] > 1 else y - y.min()) / (y.max() - y.min() + 1e-8)\n                \n                # Simple SSIM approximation\n                c1, c2 = 0.01**2, 0.03**2\n                mu_x = F.avg_pool2d(pred_norm, kernel_size=11, stride=1, padding=5)\n                mu_y = F.avg_pool2d(target_norm, kernel_size=11, stride=1, padding=5)\n                \n                sigma_x = F.avg_pool2d(pred_norm**2, kernel_size=11, stride=1, padding=5) - mu_x**2\n                sigma_y = F.avg_pool2d(target_norm**2, kernel_size=11, stride=1, padding=5) - mu_y**2\n                sigma_xy = F.avg_pool2d(pred_norm * target_norm, kernel_size=11, stride=1, padding=5) - mu_x * mu_y\n                \n                ssim = ((2 * mu_x * mu_y + c1) * (2 * sigma_xy + c2)) / \\\n                       ((mu_x**2 + mu_y**2 + c1) * (sigma_x + sigma_y + c2))\n                       \n                ssim_values += torch.mean(ssim).item()\n        \n        # Calculate average validation metrics\n        avg_val_loss = epoch_val_loss / len(val_loader)\n        avg_l1_error = l1_error / len(val_loader)\n        avg_ssim = ssim_values / len(val_loader)\n        val_losses.append(avg_val_loss)\n        \n        # Update learning rate\n        scheduler.step(avg_val_loss)\n        \n        # Print epoch results\n        epoch_time = time.time() - epoch_start\n        print(f\"Epoch {epoch}/{num_epochs} completed in {epoch_time:.2f}s | \"\n              f\"Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | \"\n              f\"L1 Error: {avg_l1_error:.6f} | SSIM: {avg_ssim:.6f}\")\n        \n        # Visualize predictions\n        if epoch % 5 == 0 or epoch == 1 or epoch == num_epochs:\n            visualize_predictions(model, val_loader, device, epoch, config)\n        \n        # Check for improvement\n        if avg_val_loss < best_val_loss:\n            improvement = (best_val_loss - avg_val_loss) / best_val_loss * 100\n            best_val_loss = avg_val_loss\n            early_stop_counter = 0\n            \n            # Save best model\n            if config['save_models']:\n                best_model_path = experiment_path / f\"best_model_epoch_{epoch}.pt\"\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'loss': best_val_loss,\n                    'train_losses': train_losses,\n                    'val_losses': val_losses\n                }, best_model_path)\n                \n                # Also save with standard name\n                torch.save(model.state_dict(), experiment_path / \"best_model.pt\")\n                \n                print(f\"✅ New best model saved with {improvement:.2f}% improvement\")\n        else:\n            early_stop_counter += 1\n            \n        # Early stopping check\n        if early_stop_counter >= early_stopping:\n            print(f\"Early stopping triggered after {epoch} epochs\")\n            break\n            \n        # Save checkpoint every 10 epochs\n        if epoch % 10 == 0 and config['save_models']:\n            checkpoint_path = experiment_path / f\"checkpoint_epoch_{epoch}.pt\"\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': avg_val_loss,\n                'train_losses': train_losses,\n                'val_losses': val_losses\n            }, checkpoint_path)\n            print(f\"Checkpoint saved at epoch {epoch}\")\n    \n    # Save final model\n    if config['save_models']:\n        final_path = experiment_path / \"final_model.pt\"\n        torch.save({\n            'model_state_dict': model.state_dict(),\n            'loss': avg_val_loss,\n            'train_losses': train_losses,\n            'val_losses': val_losses\n        }, final_path)\n        print(f\"✅ Final model saved at: {final_path}\")\n    \n    # Plot loss curves\n    plt.figure(figsize=(10, 5))\n    plt.plot(train_losses, label='Training Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title(f'{config[\"approach\"].capitalize()} Model - Loss Curves')\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(output_dir / 'visualizations' / f\"{experiment_name}_loss_curves.png\")\n    plt.close()\n    \n    # Report training summary\n    total_time = time.time() - start_time\n    print(f\"Training completed in {total_time/60:.2f} minutes\")\n    print(f\"Best validation loss: {best_val_loss:.6f}\")\n    \n    return model, train_losses, val_losses\n\n\ndef visualize_predictions(model, loader, device, epoch, config):\n    \"\"\"Generate visualizations based on the selected approach\"\"\"\n    model.eval()\n    \n    # Get a batch of data\n    x, y = next(iter(loader))\n    x, y = x.to(device), y.to(device)\n    \n    # Make predictions\n    with torch.no_grad():\n        if config['approach'] == 'feature_detection':\n            predictions = model(x)\n        else:\n            predictions = model(x)\n    \n    # Create visualization based on approach\n    if config['approach'] == 'feature_detection':\n        visualize_geological_features(x, y, predictions, epoch, config)\n    else:\n        visualize_velocity_prediction(x, y, predictions, epoch, config)\n\n\ndef visualize_velocity_prediction(x, y, pred, epoch, config):\n    \"\"\"Visualize velocity model predictions\"\"\"\n    # Convert tensors to numpy\n    x_np = x.cpu().numpy()\n    y_np = y.cpu().numpy()\n    pred_np = pred.cpu().numpy()\n    \n    # Create figure\n    n_samples = min(4, x.size(0))\n    fig, axs = plt.subplots(n_samples, 4, figsize=(16, 4 * n_samples))\n    \n    # Handle single sample case\n    if n_samples == 1:\n        axs = np.array([axs])\n    \n    for i in range(n_samples):\n        # Input seismic data (first channel)\n        axs[i, 0].imshow(x_np[i, 0], cmap='seismic', aspect='auto')\n        axs[i, 0].set_title(f\"Input Seismic #{i+1}\")\n        \n        # Ground truth velocity\n        if y_np.shape[1] > 1:\n            y_plot = y_np[i, 0]  # For feature mode, use first channel (salt)\n        else:\n            y_plot = y_np[i, 0]\n            \n        axs[i, 1].imshow(y_plot, cmap='magma', origin='lower')\n        axs[i, 1].set_title(f\"Ground Truth #{i+1}\")\n        \n        # Predicted velocity\n        axs[i, 2].imshow(pred_np[i, 0], cmap='magma', origin='lower')\n        axs[i, 2].set_title(f\"Prediction #{i+1}\")\n        \n        # Absolute error\n        abs_error = np.abs(pred_np[i, 0] - y_plot)\n        axs[i, 3].imshow(abs_error, cmap='inferno', origin='lower')\n        axs[i, 3].set_title(f\"Abs Error #{i+1}\")\n        \n        # Remove axis ticks\n        for j in range(4):\n            axs[i, j].set_xticks([])\n            axs[i, j].set_yticks([])\n    \n    plt.tight_layout()\n    plt.suptitle(f\"Physics-Guided Velocity Prediction - Epoch {epoch}\", y=1.02, fontsize=16)\n    plt.savefig(config['output_dir'] / 'visualizations' / f\"velocity_pred_epoch_{epoch}.png\", bbox_inches='tight')\n    plt.close()\n    \ndef visualize_geological_features(x, y, predictions, epoch, config):\n    \"\"\"Visualize geological feature predictions - FIXED for mixed precision compatibility\"\"\"\n    # Convert tensors to numpy\n    x_np = x.cpu().numpy()\n    y_np = y.cpu().numpy()\n    \n    # Apply sigmoid to convert logits to probabilities for visualization\n    salt_pred = torch.sigmoid(predictions['salt']).cpu().numpy()\n    fault_pred = torch.sigmoid(predictions['faults']).cpu().numpy()\n    layer_pred = torch.sigmoid(predictions['layers']).cpu().numpy()\n    velocity_pred = predictions['velocity'].cpu().numpy()\n    \n    # Create figure\n    n_samples = min(3, x.size(0))\n    fig, axs = plt.subplots(n_samples, 5, figsize=(20, 5 * n_samples))\n    \n    # Handle single sample case\n    if n_samples == 1:\n        axs = np.array([axs])\n    \n    for i in range(n_samples):\n        # Input seismic data (first channel)\n        axs[i, 0].imshow(x_np[i, 0], cmap='seismic', aspect='auto')\n        axs[i, 0].set_title(f\"Input Seismic #{i+1}\")\n        \n        # Salt body detection\n        salt_true = y_np[i, 0]\n        axs[i, 1].imshow(salt_pred[i, 0], cmap='viridis')\n        axs[i, 1].contour(salt_true, colors='r', linewidths=0.5, levels=[0.5])\n        axs[i, 1].set_title(f\"Salt Detection #{i+1}\")\n        \n        # Fault detection\n        fault_true = y_np[i, 1]\n        axs[i, 2].imshow(fault_pred[i, 0], cmap='viridis')\n        axs[i, 2].contour(fault_true, colors='r', linewidths=0.5, levels=[0.5])\n        axs[i, 2].set_title(f\"Fault Detection #{i+1}\")\n        \n        # Layer detection\n        layer_true = y_np[i, 2]\n        axs[i, 3].imshow(layer_pred[i, 0], cmap='viridis')\n        axs[i, 3].contour(layer_true, colors='r', linewidths=0.5, levels=[0.5])\n        axs[i, 3].set_title(f\"Layer Detection #{i+1}\")\n        \n        # Reconstructed velocity\n        axs[i, 4].imshow(velocity_pred[i, 0], cmap='magma', origin='lower')\n        axs[i, 4].set_title(f\"Reconstructed Velocity #{i+1}\")\n        \n        # Remove axis ticks\n        for j in range(5):\n            axs[i, j].set_xticks([])\n            axs[i, j].set_yticks([])\n    \n    plt.tight_layout()\n    plt.suptitle(f\"Geological Feature Detection - Epoch {epoch}\", y=1.02, fontsize=16)\n    plt.savefig(config['output_dir'] / 'visualizations' / f\"features_epoch_{epoch}.png\", bbox_inches='tight')\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T21:55:53.87366Z","iopub.execute_input":"2025-04-30T21:55:53.873898Z","iopub.status.idle":"2025-04-30T21:55:53.906711Z","shell.execute_reply.started":"2025-04-30T21:55:53.873882Z","shell.execute_reply":"2025-04-30T21:55:53.906192Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Cell 6: Threshold-Based Approach\n\ndef apply_thresholding(data, method='otsu', threshold=None, stats=None):\n    \"\"\"Apply thresholding to create binary velocity model\"\"\"\n    from skimage import filters\n    \n    # Determine threshold\n    if threshold is not None:\n        # Use provided threshold\n        thresh = threshold\n    elif method == 'otsu':\n        # Otsu's adaptive thresholding\n        try:\n            thresh = filters.threshold_otsu(data)\n        except:\n            # Fallback to mean if Otsu fails\n            thresh = np.mean(data)\n    elif method == 'mean':\n        # Mean-based thresholding\n        thresh = np.mean(data)\n    elif method == 'adaptive':\n        # Local adaptive thresholding\n        thresh = filters.threshold_local(data, block_size=15)\n    elif stats is not None:\n        # Use pre-calculated statistics\n        thresh = stats['auto_threshold']\n    else:\n        # Default to mean\n        thresh = np.mean(data)\n    \n    # Apply threshold\n    binary = (data > thresh).astype(np.float32)\n    \n    return binary\n\n\ndef post_process_binary(binary, config):\n    \"\"\"Apply post-processing to binary velocity model - FIXED for dimension mismatch\"\"\"\n    from scipy import ndimage\n    \n    # Copy to avoid modifying original\n    result = binary.copy()\n    \n    # Check input dimensions\n    input_dims = len(result.shape)\n    \n    # Apply morphological operations if enabled\n    if config.get('use_morphology', True):\n        # Create structure element with matching dimensions\n        if input_dims == 2:\n            # 2D input\n            structure = np.ones((2, 2))\n            structure_close = np.ones((3, 3))\n        elif input_dims == 3:\n            # 3D input (batch, height, width)\n            if result.shape[0] == 1:\n                # Single channel, treat as 2D after squeezing\n                result = result.squeeze(0)\n                structure = np.ones((2, 2))\n                structure_close = np.ones((3, 3))\n                # Flag that we need to unsqueeze later\n                needs_unsqueeze = True\n            else:\n                # Multi-batch, use 3D structure\n                structure = np.ones((1, 2, 2))\n                structure_close = np.ones((1, 3, 3))\n                needs_unsqueeze = False\n        elif input_dims == 4:\n            # 4D input (batch, channels, height, width)\n            structure = np.ones((1, 1, 2, 2))\n            structure_close = np.ones((1, 1, 3, 3))\n            needs_unsqueeze = False\n        else:\n            # Unexpected dimension, skip morphology\n            print(f\"Warning: Skipping morphology for unusual dimensions: {result.shape}\")\n            structure = None\n            \n        # Apply morphology if we have a valid structure\n        if structure is not None:\n            try:\n                # Clean up small artifacts\n                result = ndimage.binary_opening(result, structure=structure)\n                result = ndimage.binary_closing(result, structure=structure_close)\n                \n                # Restore original dimensions if needed\n                if input_dims == 3 and 'needs_unsqueeze' in locals() and needs_unsqueeze:\n                    result = result[np.newaxis, ...]\n            except Exception as e:\n                print(f\"Warning: Morphology operation failed: {e}\")\n                print(f\"Input shape: {binary.shape}, Structure shape: {structure.shape}\")\n    \n    # Apply edge enhancement if specified\n    if config.get('edge_enhancement', 0) > 0:\n        # Calculate edges\n        edge_x = ndimage.sobel(binary, axis=-2)\n        edge_y = ndimage.sobel(binary, axis=-1)\n        edges = np.sqrt(edge_x**2 + edge_y**2)\n        \n        # Normalize edge strength\n        edge_strength = edges / (np.max(edges) + 1e-8)\n        \n        # Enhance edges\n        enhanced = result.copy()\n        enhanced[edge_strength > 0.3] = 1.0\n        \n        # Blend with original based on enhancement strength\n        alpha = config['edge_enhancement']\n        result = (1 - alpha) * result + alpha * enhanced\n    \n    return result\n\n\ndef process_with_thresholding(input_data, config, stats=None):\n    \"\"\"Process seismic data using thresholding approach\"\"\"\n    # Apply gain to seismic data\n    time_steps = input_data.shape[1]\n    time = np.linspace(0, 1, time_steps)\n    gain = (time ** 2)[:, np.newaxis]\n    \n    gained_data = input_data.copy()\n    for c in range(input_data.shape[0]):\n        gained_data[c] = input_data[c] * gain\n    \n    # Take mean across channels and time\n    # We need a 2D map that we can threshold\n    mean_data = np.mean(gained_data, axis=(0, 1))\n    \n    # Apply thresholding\n    binary = apply_thresholding(\n        mean_data, \n        method=config['threshold_method'],\n        stats=stats\n    )\n    \n    # Apply post-processing\n    processed = post_process_binary(binary, config)\n    \n    return processed\n\n\ndef generate_threshold_submission(test_dir, output_path, config, stats=None):\n    \"\"\"Generate submission using thresholding approach\"\"\"\n    # Get test files\n    test_files = sorted(glob.glob(os.path.join(test_dir, \"*.npy\")))\n    print(f\"Found {len(test_files)} test files\")\n    \n    rows = []\n    \n    # Process files with progress tracking\n    progress = tqdm(test_files, desc=\"Generating threshold-based submission\")\n    \n    for filepath in progress:\n        # Get file ID\n        oid = os.path.splitext(os.path.basename(filepath))[0]\n        \n        # Load data\n        data = np.load(filepath)\n        \n        # Check shape and extract if needed\n        if len(data.shape) == 4:  # (batch, channels, time, receivers)\n            data = data[0]  # Use first sample if batched\n        \n        # Process with thresholding\n        binary_pred = process_with_thresholding(data, config, stats)\n        \n        # Ensure correct shape (70x70)\n        if binary_pred.shape != (70, 70):\n            from skimage.transform import resize\n            binary_pred = resize(binary_pred, (70, 70), order=0, preserve_range=True)\n        \n        # Format for submission (all rows, odd columns)\n        for y in range(70):\n            row_id = f\"{oid}_y_{y}\"\n            row_data = binary_pred[y, 1:70:2]  # Extract odd-indexed columns\n            rows.append([row_id] + row_data.tolist())\n    \n    # Create submission DataFrame\n    columns = [\"ID\"] + [f\"x_{i}\" for i in range(1, 70, 2)]\n    submission_df = pd.DataFrame(rows, columns=columns)\n    \n    # Save submission\n    submission_df.to_csv(output_path, index=False)\n    print(f\"✅ Threshold-based submission saved at: {output_path}\")\n    \n    return submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T21:55:53.907578Z","iopub.execute_input":"2025-04-30T21:55:53.907828Z","iopub.status.idle":"2025-04-30T21:55:53.927723Z","shell.execute_reply.started":"2025-04-30T21:55:53.907805Z","shell.execute_reply":"2025-04-30T21:55:53.927015Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Cell 7: Inference and Submission Functions\n\ndef load_best_model(config):\n    \"\"\"Load the best model for inference\"\"\"\n    # Path to best model\n    model_path = config['output_dir'] / 'models' / experiment_name / \"best_model.pt\"\n    standard_path = config['output_dir'] / 'models' / experiment_name / \"best_model.pt\"\n    \n    # Create model\n    model = create_model(config)\n    \n    # Try to load model weights\n    try:\n        # First try the full checkpoint format\n        checkpoint = torch.load(model_path, map_location=config['device'])\n        if 'model_state_dict' in checkpoint:\n            model.load_state_dict(checkpoint['model_state_dict'])\n            print(f\"Loaded model from checkpoint at epoch {checkpoint.get('epoch', 'unknown')}\")\n        else:\n            # If not a checkpoint, try direct state dict\n            model.load_state_dict(checkpoint)\n            print(f\"Loaded model state dict from {model_path}\")\n    except FileNotFoundError:\n        try:\n            # Try standard path as fallback\n            model.load_state_dict(torch.load(standard_path, map_location=config['device']))\n            print(f\"Loaded model from standard path: {standard_path}\")\n        except FileNotFoundError:\n            print(f\"Warning: No model found at {model_path} or {standard_path}\")\n            print(\"Using untrained model\")\n    \n    # Set to evaluation mode\n    model.eval()\n    \n    return model\n\n\ndef post_process_prediction(pred_np, config, stats=None):\n    \"\"\"Apply post-processing to model predictions\"\"\"\n    # Determine approach-specific processing\n    if config['approach'] == 'feature_detection':\n        # For feature detection, use the reconstructed velocity\n        velocity = pred_np['velocity']\n        if config['post_process']:\n            # Apply binary thresholding to get clean velocity model\n            binary = (velocity > 0.5).astype(np.float32)\n            processed = post_process_binary(binary, config)\n            return processed\n        else:\n            # Return raw velocity reconstruction\n            return velocity\n            \n    elif config['approach'] == 'thresholding':\n        # For thresholding, apply edge enhancement\n        if config['post_process']:\n            return post_process_binary(pred_np, config)\n        else:\n            return pred_np\n            \n    else:  # physics_guided\n        # For physics-guided approach\n        if config['post_process']:\n            # Enhance contrast for clearer boundaries\n            from scipy import ndimage\n            \n            # Smooth with edge-preserving bilateral filter if available\n            try:\n                from skimage.restoration import denoise_bilateral\n                smoothed = denoise_bilateral(\n                    pred_np.squeeze(), \n                    sigma_color=0.1, \n                    sigma_spatial=1,\n                    multichannel=False\n                )\n            except:\n                # Fallback to gaussian filter\n                smoothed = ndimage.gaussian_filter(pred_np.squeeze(), sigma=0.5)\n            \n            # Apply contrast adjustment\n            p2, p98 = np.percentile(smoothed, (2, 98))\n            enhanced = np.clip((smoothed - p2) / (p98 - p2), 0, 1)\n            \n            # Convert to binary if very high contrast\n            hist, bins = np.histogram(enhanced, bins=50)\n            if np.max(hist) > 0.4 * np.sum(hist):\n                # Likely bimodal distribution, apply thresholding\n                thresh = apply_thresholding(enhanced, method=config['threshold_method'], stats=stats)\n                processed = post_process_binary(thresh, config)\n                return processed\n            else:\n                # Not clearly bimodal, return enhanced version\n                return enhanced\n        else:\n            # Return raw prediction\n            return pred_np.squeeze()\n\n\ndef generate_model_submission(model, test_dir, output_path, config, stats=None):\n    \"\"\"Generate submission using trained model\"\"\"\n    # Get test files\n    test_files = sorted(glob.glob(os.path.join(test_dir, \"*.npy\")))\n    print(f\"Found {len(test_files)} test files\")\n    \n    rows = []\n    device = config['device']\n    \n    # Process files with progress tracking\n    progress = tqdm(test_files, desc=f\"Generating {config['approach']} submission\")\n    \n    with torch.no_grad():\n        for filepath in progress:\n            # Get file ID\n            oid = os.path.splitext(os.path.basename(filepath))[0]\n            \n            # Load data\n            data = np.load(filepath)\n            \n            # Check shape and extract if needed\n            if len(data.shape) == 4:  # (batch, channels, time, receivers)\n                data = data[0]  # Use first sample if batched\n            \n            # Apply gain\n            time_steps = data.shape[1]\n            time = np.linspace(0, 1, time_steps)\n            gain = (time ** 2)[:, np.newaxis]\n            \n            gained_data = data.copy()\n            for c in range(data.shape[0]):\n                gained_data[c] = data[c] * gain\n                \n            # Normalize if statistics are available\n            if stats:\n                gained_data = (gained_data - stats['input_mean']) / (stats['input_std'] + 1e-6)\n            \n            # Convert to tensor and add batch dimension\n            x = torch.from_numpy(gained_data).float().unsqueeze(0).to(device)\n            \n            # Predict with model\n            if config['approach'] == 'feature_detection':\n                pred = model(x)\n                # Convert dict of tensors to dict of numpy arrays\n                pred_np = {k: v.cpu().numpy() for k, v in pred.items()}\n            else:\n                pred = model(x)\n                pred_np = pred.cpu().numpy()\n            \n            # Apply post-processing\n            processed = post_process_prediction(pred_np, config, stats)\n            \n            # Ensure correct shape (70x70)\n            if processed.shape != (70, 70):\n                processed = processed.squeeze()\n                if processed.shape != (70, 70):\n                    from skimage.transform import resize\n                    processed = resize(processed, (70, 70), order=1, preserve_range=True)\n            \n            # Format for submission (all rows, odd columns)\n            for y in range(70):\n                row_id = f\"{oid}_y_{y}\"\n                row_data = processed[y, 1:70:2]  # Extract odd-indexed columns\n                rows.append([row_id] + row_data.tolist())\n    \n    # Create submission DataFrame\n    columns = [\"ID\"] + [f\"x_{i}\" for i in range(1, 70, 2)]\n    submission_df = pd.DataFrame(rows, columns=columns)\n    \n    # Save submission\n    submission_df.to_csv(output_path, index=False)\n    print(f\"✅ Model-based submission saved at: {output_path}\")\n    \n    return submission_df\n\n\ndef ensemble_predictions(models, x, config):\n    \"\"\"Generate ensemble prediction from multiple models\"\"\"\n    device = config['device']\n    ensemble_preds = []\n    \n    with torch.no_grad():\n        # Basic predictions\n        for model in models:\n            if config['approach'] == 'feature_detection':\n                pred = model(x)\n                # Use velocity reconstruction\n                ensemble_preds.append(pred['velocity'])\n            else:\n                pred = model(x)\n                ensemble_preds.append(pred)\n                \n            # Add test-time augmentation if enabled\n            if config.get('use_tta', False):\n                # Horizontal flip\n                x_flip = torch.flip(x, dims=[3])\n                if config['approach'] == 'feature_detection':\n                    pred_flip = model(x_flip)\n                    # Flip back the prediction\n                    velocity_flip = torch.flip(pred_flip['velocity'], dims=[3])\n                    ensemble_preds.append(velocity_flip)\n                else:\n                    pred_flip = model(x_flip)\n                    pred_flip = torch.flip(pred_flip, dims=[3])\n                    ensemble_preds.append(pred_flip)\n    \n    # Average all predictions\n    ensemble_pred = torch.mean(torch.stack(ensemble_preds), dim=0)\n    \n    return ensemble_pred\n\n\ndef generate_ensemble_submission(models, test_dir, output_path, config, stats=None):\n    \"\"\"Generate submission using ensemble of models\"\"\"\n    # Get test files\n    test_files = sorted(glob.glob(os.path.join(test_dir, \"*.npy\")))\n    print(f\"Found {len(test_files)} test files\")\n    \n    rows = []\n    device = config['device']\n    \n    # Process files with progress tracking\n    progress = tqdm(test_files, desc=\"Generating ensemble submission\")\n    \n    with torch.no_grad():\n        for filepath in progress:\n            # Get file ID\n            oid = os.path.splitext(os.path.basename(filepath))[0]\n            \n            # Load data\n            data = np.load(filepath)\n            \n            # Check shape and extract if needed\n            if len(data.shape) == 4:  # (batch, channels, time, receivers)\n                data = data[0]  # Use first sample if batched\n            \n            # Apply gain\n            time_steps = data.shape[1]\n            time = np.linspace(0, 1, time_steps)\n            gain = (time ** 2)[:, np.newaxis]\n            \n            gained_data = data.copy()\n            for c in range(data.shape[0]):\n                gained_data[c] = data[c] * gain\n                \n            # Normalize if statistics are available\n            if stats:\n                gained_data = (gained_data - stats['input_mean']) / (stats['input_std'] + 1e-6)\n            \n            # Convert to tensor and add batch dimension\n            x = torch.from_numpy(gained_data).float().unsqueeze(0).to(device)\n            \n            # Get ensemble prediction\n            ensemble_pred = ensemble_predictions(models, x, config)\n            pred_np = ensemble_pred.cpu().numpy()\n            \n            # Apply post-processing\n            processed = post_process_prediction(pred_np, config, stats)\n            \n            # Ensure correct shape (70x70)\n            if processed.shape != (70, 70):\n                processed = processed.squeeze()\n                if processed.shape != (70, 70):\n                    from skimage.transform import resize\n                    processed = resize(processed, (70, 70), order=1, preserve_range=True)\n            \n            # Format for submission (all rows, odd columns)\n            for y in range(70):\n                row_id = f\"{oid}_y_{y}\"\n                row_data = processed[y, 1:70:2]  # Extract odd-indexed columns\n                rows.append([row_id] + row_data.tolist())\n    \n    # Create submission DataFrame\n    columns = [\"ID\"] + [f\"x_{i}\" for i in range(1, 70, 2)]\n    submission_df = pd.DataFrame(rows, columns=columns)\n    \n    # Save submission\n    submission_df.to_csv(output_path, index=False)\n    print(f\"✅ Ensemble submission saved at: {output_path}\")\n    \n    return submission_df\n\n\ndef generate_submission(config, stats=None):\n    \"\"\"Generate submission based on selected approach\"\"\"\n    approach = config['approach']\n    output_path = config['submission_path']\n    \n    if approach == 'thresholding':\n        # Use direct thresholding approach (no training)\n        return generate_threshold_submission(\n            config['test_dir'], \n            output_path, \n            config, \n            stats\n        )\n    elif config['ensemble_submission']:\n        # Load multiple models for ensemble\n        model_dir = config['output_dir'] / 'models' / experiment_name\n        model_files = list(model_dir.glob(\"*model_epoch_*.pt\"))\n        \n        if len(model_files) > 1:\n            print(f\"Found {len(model_files)} models for ensemble\")\n            models = []\n            \n            for model_file in model_files:\n                model = create_model(config)\n                # Load checkpoint\n                checkpoint = torch.load(model_file, map_location=config['device'])\n                if 'model_state_dict' in checkpoint:\n                    model.load_state_dict(checkpoint['model_state_dict'])\n                else:\n                    model.load_state_dict(checkpoint)\n                    \n                model.eval()\n                models.append(model)\n                \n            return generate_ensemble_submission(\n                models,\n                config['test_dir'],\n                output_path,\n                config,\n                stats\n            )\n        else:\n            # Fall back to single model if not enough models found\n            print(\"Not enough models for ensemble, using best model\")\n            model = load_best_model(config)\n            return generate_model_submission(\n                model, \n                config['test_dir'], \n                output_path, \n                config, \n                stats\n            )\n    else:\n        # Use single best model\n        model = load_best_model(config)\n        return generate_model_submission(\n            model, \n            config['test_dir'], \n            output_path, \n            config, \n            stats\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T21:55:53.928668Z","iopub.execute_input":"2025-04-30T21:55:53.928925Z","iopub.status.idle":"2025-04-30T21:55:53.957109Z","shell.execute_reply.started":"2025-04-30T21:55:53.928902Z","shell.execute_reply":"2025-04-30T21:55:53.956319Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Cell 8: Run Main Pipeline\n\ndef run_pipeline(config):\n    \"\"\"Run the complete pipeline based on configuration\"\"\"\n    # 1. Prepare datasets\n    train_loader, val_loader, stats = load_and_prepare_data(config)\n    \n    # 2. Create model and loss function\n    if config['approach'] != 'thresholding':\n        model = create_model(config)\n        criterion = create_loss_function(config)\n        \n        # 3. Train model\n        model, train_losses, val_losses = train_model(\n            model, train_loader, val_loader, criterion, config\n        )\n    else:\n        print(\"Using thresholding approach, skipping model training\")\n        model = None\n    \n    # 4. Generate submission\n    submission_df = generate_submission(config, stats)\n    \n    print(f\"Pipeline completed for {config['approach']} approach\")\n    print(f\"Submission saved at: {config['submission_path']}\")\n    \n    return submission_df\n\n\n# Execute the pipeline\nif __name__ == \"__main__\":\n    # Run the selected approach\n    submission_df = run_pipeline(CONFIG)\n    \n    # If comparison mode is enabled, run multiple approaches\n    if CONFIG['compare_approaches']:\n        approaches = ['thresholding', 'physics_guided', 'feature_detection']\n        results = {}\n        \n        for approach in approaches:\n            if approach != CONFIG['approach']:  # Skip already run approach\n                print(f\"\\nRunning comparison pipeline for {approach} approach\")\n                \n                # Create copy of config with new approach\n                comp_config = CONFIG.copy()\n                comp_config['approach'] = approach\n                comp_config['submission_path'] = f\"submission_{approach}.csv\"\n                \n                # Run pipeline with this approach\n                results[approach] = run_pipeline(comp_config)\n        \n        # Show comparison summary\n        print(\"\\nComparison of approaches completed:\")\n        for approach in approaches:\n            path = f\"submission_{approach}.csv\" if approach != CONFIG['approach'] else CONFIG['submission_path']\n            print(f\"- {approach.capitalize()}: {path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T21:55:53.959012Z","iopub.execute_input":"2025-04-30T21:55:53.959231Z","iopub.status.idle":"2025-04-30T22:12:57.849168Z","shell.execute_reply.started":"2025-04-30T21:55:53.959204Z","shell.execute_reply":"2025-04-30T22:12:57.845043Z"}},"outputs":[{"name":"stdout","text":"Loading competition dataset...\nFound 20 input files and 20 output files\nTraining set: 17 files\nValidation set: 3 files\nCalculating dataset statistics...\nInput stats: mean=-0.0001, std=1.4956\nOutput stats: mean=2905.3801, std=819.7678\nAuto threshold: 2936.0684\nCalculating dataset statistics...\nInput stats: mean=-0.0001, std=1.5977\nOutput stats: mean=2912.9771, std=794.2023\nAuto threshold: 2951.0947\nCreated Geological Feature Detection model\nModel has 6,264,260 parameters (6,264,260 trainable)\nCreated Geological Feature Loss\nStarting training for 2 epochs...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2 [Train]: 100%|██████████| 532/532 [01:31<00:00,  5.82it/s, loss=3.36]\nEpoch 1/2 [Valid]: 100%|██████████| 94/94 [00:08<00:00, 11.16it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/2 completed in 99.85s | Train Loss: 2.971327 | Val Loss: 3.092229 | L1 Error: 0.500414 | SSIM: 0.113866\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"✅ New best model saved with nan% improvement\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2 [Train]: 100%|██████████| 532/532 [01:29<00:00,  5.97it/s, loss=2.94]\nEpoch 2/2 [Valid]: 100%|██████████| 94/94 [00:07<00:00, 11.92it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 2/2 completed in 97.07s | Train Loss: 2.749588 | Val Loss: 2.985715 | L1 Error: 0.507478 | SSIM: 0.101712\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"✅ New best model saved with 3.44% improvement\n✅ Final model saved at: outputs/models/feature_detection_20250430_215553/final_model.pt\nTraining completed in 3.35 minutes\nBest validation loss: 2.985715\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_279/928907769.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(model_path, map_location=config['device'])\n","output_type":"stream"},{"name":"stdout","text":"Created Geological Feature Detection model\nModel has 6,264,260 parameters (6,264,260 trainable)\nLoaded model state dict from outputs/models/feature_detection_20250430_215553/best_model.pt\nFound 65818 test files\n","output_type":"stream"},{"name":"stderr","text":"Generating feature_detection submission:  43%|████▎     | 28618/65818 [13:39<17:44, 34.94it/s]  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_279/360919344.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Run the selected approach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0msubmission_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# If comparison mode is enabled, run multiple approaches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_279/360919344.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# 4. Generate submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0msubmission_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Pipeline completed for {config['approach']} approach\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_279/928907769.py\u001b[0m in \u001b[0;36mgenerate_submission\u001b[0;34m(config, stats)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;31m# Use single best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_best_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         return generate_model_submission(\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_279/928907769.py\u001b[0m in \u001b[0;36mgenerate_model_submission\u001b[0;34m(model, test_dir, output_path, config, stats)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;31m# Convert dict of tensors to dict of numpy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0mpred_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_279/928907769.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;31m# Convert dict of tensors to dict of numpy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0mpred_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"# Cell 9: Additional Analysis and Visualization Functions (Optional)\n\ndef analyze_velocity_models(output_files, max_samples=10):\n    \"\"\"Analyze velocity models to understand their characteristics\"\"\"\n    # Load a subset of velocity models\n    sample_files = output_files[:min(len(output_files), max_samples)]\n    samples = []\n    \n    for f in sample_files:\n        try:\n            data = np.load(f, mmap_mode='r')\n            if len(data.shape) == 4:  # [batch, channel, height, width]\n                samples.append(data[:5])  # Take first 5 samples\n            elif len(data.shape) == 3:  # [batch, height, width]\n                samples.append(data[:5, np.newaxis])  # Add channel dimension\n        except Exception as e:\n            print(f\"Error loading {f}: {e}\")\n    \n    if not samples:\n        print(\"No samples loaded for analysis\")\n        return\n        \n    # Concatenate samples\n    all_samples = np.concatenate(samples, axis=0).squeeze()\n    print(f\"Analyzing {all_samples.shape[0]} velocity models\")\n    \n    # Calculate statistics\n    mean_vel = np.mean(all_samples)\n    std_vel = np.std(all_samples)\n    min_vel = np.min(all_samples)\n    max_vel = np.max(all_samples)\n    \n    print(f\"Velocity statistics: mean={mean_vel:.2f}, std={std_vel:.2f}, min={min_vel:.2f}, max={max_vel:.2f}\")\n    \n    # Histogram analysis\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(131)\n    plt.hist(all_samples.flatten(), bins=50)\n    plt.title(\"Velocity Distribution\")\n    plt.xlabel(\"Velocity Value\")\n    plt.ylabel(\"Frequency\")\n    \n    # Check if distribution is bimodal\n    from scipy import stats as scipy_stats\n    kde = scipy_stats.gaussian_kde(all_samples.flatten())\n    x = np.linspace(min_vel, max_vel, 1000)\n    plt.plot(x, kde(x) * len(all_samples.flatten()) * (max_vel - min_vel) / 50, 'r-', linewidth=2)\n    \n    # Gradient analysis\n    grad_y, grad_x = np.gradient(all_samples[0])\n    grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n    \n    plt.subplot(132)\n    plt.hist(grad_mag.flatten(), bins=50)\n    plt.title(\"Gradient Magnitude Distribution\")\n    plt.xlabel(\"Gradient Magnitude\")\n    plt.ylabel(\"Frequency\")\n    \n    # Layer analysis\n    plt.subplot(133)\n    \n    # Average over horizontal slices to see layering\n    layer_profile = np.mean(all_samples, axis=(0, 2))\n    plt.plot(layer_profile, np.arange(len(layer_profile)))\n    plt.title(\"Average Horizontal Layer Profile\")\n    plt.xlabel(\"Average Velocity\")\n    plt.ylabel(\"Depth\")\n    plt.gca().invert_yaxis()  # Depth increases downward\n    \n    plt.tight_layout()\n    plt.savefig(CONFIG['output_dir'] / 'visualizations' / \"velocity_analysis.png\")\n    plt.show()\n    \n    # Display example velocity models with edge detection\n    plt.figure(figsize=(15, 5 * min(3, len(all_samples))))\n    \n    for i in range(min(3, len(all_samples))):\n        # Original velocity model\n        plt.subplot(min(3, len(all_samples)), 3, i*3 + 1)\n        plt.imshow(all_samples[i], cmap='magma', origin='lower')\n        plt.title(f\"Velocity Model #{i+1}\")\n        plt.colorbar()\n        \n        # Edge detection\n        from scipy import ndimage\n        grad_y, grad_x = np.gradient(all_samples[i])\n        edges = np.sqrt(grad_x**2 + grad_y**2)\n        \n        plt.subplot(min(3, len(all_samples)), 3, i*3 + 2)\n        plt.imshow(edges, cmap='viridis', origin='lower')\n        plt.title(f\"Edge Detection #{i+1}\")\n        plt.colorbar()\n        \n        # Binary thresholding\n        from skimage import filters\n        try:\n            thresh = filters.threshold_otsu(all_samples[i])\n        except:\n            thresh = np.mean(all_samples[i])\n            \n        binary = (all_samples[i] > thresh).astype(float)\n        \n        plt.subplot(min(3, len(all_samples)), 3, i*3 + 3)\n        plt.imshow(binary, cmap='gray', origin='lower')\n        plt.title(f\"Binary Threshold (t={thresh:.2f}) #{i+1}\")\n        \n    plt.tight_layout()\n    plt.savefig(CONFIG['output_dir'] / 'visualizations' / \"velocity_examples.png\")\n    plt.show()\n    \n    return {\n        'mean': mean_vel,\n        'std': std_vel,\n        'min': min_vel,\n        'max': max_vel,\n        'bimodal': scipy_stats.bimodality_coefficient(all_samples.flatten()) > 0.555  # BC threshold for bimodality\n    }\n\n\ndef visualize_seismic_data(input_files, max_samples=5):\n    \"\"\"Visualize seismic data to understand input characteristics\"\"\"\n    # Load a subset of seismic data files\n    sample_files = input_files[:min(len(input_files), max_samples)]\n    samples = []\n    \n    for f in sample_files:\n        try:\n            data = np.load(f, mmap_mode='r')\n            samples.append(data[:1])  # Take first sample from each file\n        except Exception as e:\n            print(f\"Error loading {f}: {e}\")\n    \n    if not samples:\n        print(\"No samples loaded for visualization\")\n        return\n        \n    # Concatenate samples\n    all_samples = np.concatenate(samples, axis=0)\n    print(f\"Visualizing {all_samples.shape[0]} seismic data samples\")\n    \n    # Display seismic data\n    plt.figure(figsize=(15, 4 * min(3, len(all_samples))))\n    \n    for i in range(min(3, len(all_samples))):\n        # Raw seismic data (first channel)\n        plt.subplot(min(3, len(all_samples)), 3, i*3 + 1)\n        plt.imshow(all_samples[i, 0], cmap='seismic', aspect='auto')\n        plt.title(f\"Raw Seismic (Ch 1) #{i+1}\")\n        plt.colorbar()\n        \n        # Apply gain\n        time_steps = all_samples.shape[2]\n        time = np.linspace(0, 1, time_steps)\n        gain = (time ** 2)[:, np.newaxis]\n        gained = all_samples[i, 0] * gain\n        \n        plt.subplot(min(3, len(all_samples)), 3, i*3 + 2)\n        plt.imshow(gained, cmap='seismic', aspect='auto')\n        plt.title(f\"With Time Gain #{i+1}\")\n        plt.colorbar()\n        \n        # Frequency analysis\n        from scipy import fft\n        freq = np.abs(fft.fft2(all_samples[i, 0]))\n        freq = np.fft.fftshift(freq)\n        \n        plt.subplot(min(3, len(all_samples)), 3, i*3 + 3)\n        plt.imshow(np.log(freq + 1), cmap='viridis', aspect='auto')\n        plt.title(f\"Frequency Spectrum #{i+1}\")\n        plt.colorbar()\n        \n    plt.tight_layout()\n    plt.savefig(CONFIG['output_dir'] / 'visualizations' / \"seismic_visualization.png\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T22:12:57.852827Z","iopub.status.idle":"2025-04-30T22:12:57.853135Z","shell.execute_reply.started":"2025-04-30T22:12:57.852999Z","shell.execute_reply":"2025-04-30T22:12:57.853016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10: Interactive Experiment Manager (Optional)\n\ndef print_config():\n    \"\"\"Print current configuration in formatted way\"\"\"\n    print(\"\\n=== Current Configuration ===\")\n    for key, value in CONFIG.items():\n        print(f\"{key}: {value}\")\n    print(\"============================\\n\")\n\n\ndef update_config(updates):\n    \"\"\"Update configuration with new values\"\"\"\n    for key, value in updates.items():\n        if key in CONFIG:\n            CONFIG[key] = value\n        else:\n            print(f\"Warning: Unknown config key '{key}'\")\n    \n    # Update derived values\n    if 'device' in updates:\n        CONFIG['device'] = torch.device(CONFIG['device'])\n        \n    print_config()\n\n\ndef experiment_manager():\n    \"\"\"Interactive experiment manager for running different configurations\"\"\"\n    from IPython.display import clear_output\n    \n    print(\"\\n=== Geophysical Experiment Manager ===\")\n    print(\"This tool helps run experiments with different configurations\")\n    \n    while True:\n        print(\"\\nOptions:\")\n        print(\"1. View current configuration\")\n        print(\"2. Run current configuration\")\n        print(\"3. Change approach\")\n        print(\"4. Modify training parameters\")\n        print(\"5. Modify model parameters\")\n        print(\"6. Analyze data\")\n        print(\"7. Generate submission\")\n        print(\"8. Compare approaches\")\n        print(\"9. Exit\")\n        \n        choice = input(\"\\nEnter your choice (1-9): \")\n        \n        if choice == '1':\n            print_config()\n            \n        elif choice == '2':\n            clear_output(wait=True)\n            print(\"Running pipeline with current configuration...\")\n            run_pipeline(CONFIG)\n            \n        elif choice == '3':\n            print(\"\\nAvailable approaches:\")\n            print(\"1. thresholding - Simple threshold-based approach (no training)\")\n            print(\"2. physics_guided - Physics-guided neural network\")\n            print(\"3. feature_detection - Geological feature detection\")\n            \n            approach_choice = input(\"Select approach (1-3): \")\n            if approach_choice == '1':\n                update_config({'approach': 'thresholding'})\n            elif approach_choice == '2':\n                update_config({'approach': 'physics_guided'})\n            elif approach_choice == '3':\n                update_config({'approach': 'feature_detection'})\n                \n        elif choice == '4':\n            print(\"\\nModify training parameters:\")\n            try:\n                epochs = int(input(\"Number of epochs (current: {}): \".format(CONFIG['num_epochs'])) or CONFIG['num_epochs'])\n                batch_size = int(input(\"Batch size (current: {}): \".format(CONFIG['batch_size'])) or CONFIG['batch_size'])\n                lr = float(input(\"Learning rate (current: {}): \".format(CONFIG['learning_rate'])) or CONFIG['learning_rate'])\n                \n                update_config({\n                    'num_epochs': epochs,\n                    'batch_size': batch_size,\n                    'learning_rate': lr\n                })\n            except ValueError:\n                print(\"Invalid input. Please enter numeric values.\")\n                \n        elif choice == '5':\n            print(\"\\nModify model parameters:\")\n            if CONFIG['approach'] == 'physics_guided':\n                try:\n                    wave_eq = float(input(\"Wave equation weight (current: {}): \".format(CONFIG['wave_eq_weight'])) or CONFIG['wave_eq_weight'])\n                    slowness = float(input(\"Slowness weight (current: {}): \".format(CONFIG['slowness_weight'])) or CONFIG['slowness_weight'])\n                    layering = float(input(\"Layering weight (current: {}): \".format(CONFIG['layering_weight'])) or CONFIG['layering_weight'])\n                    contrast = float(input(\"Contrast weight (current: {}): \".format(CONFIG['contrast_weight'])) or CONFIG['contrast_weight'])\n                    \n                    update_config({\n                        'wave_eq_weight': wave_eq,\n                        'slowness_weight': slowness,\n                        'layering_weight': layering,\n                        'contrast_weight': contrast\n                    })\n                except ValueError:\n                    print(\"Invalid input. Please enter numeric values.\")\n            elif CONFIG['approach'] == 'feature_detection':\n                try:\n                    salt = float(input(\"Salt weight (current: {}): \".format(CONFIG['salt_weight'])) or CONFIG['salt_weight'])\n                    fault = float(input(\"Fault weight (current: {}): \".format(CONFIG['fault_weight'])) or CONFIG['fault_weight'])\n                    layer = float(input(\"Layer weight (current: {}): \".format(CONFIG['layer_weight'])) or CONFIG['layer_weight'])\n                    geo = float(input(\"Geological constraint weight (current: {}): \".format(CONFIG['geological_constraint_weight'])) or CONFIG['geological_constraint_weight'])\n                    \n                    update_config({\n                        'salt_weight': salt,\n                        'fault_weight': fault,\n                        'layer_weight': layer,\n                        'geological_constraint_weight': geo\n                    })\n                except ValueError:\n                    print(\"Invalid input. Please enter numeric values.\")\n            else:\n                print(\"No specific model parameters for threshold approach\")\n                \n        elif choice == '6':\n            print(\"\\nAnalyzing data...\")\n            # Load a small subset of data\n            input_files = sorted([f for f in CONFIG['data_dir'].rglob(\"*.npy\") \n                                 if 'seis' in f.name or 'data' in f.name])[:10]\n            output_files = [Path(str(f).replace(\"seis\", \"vel\").replace(\"data\", \"model\")) \n                          for f in input_files]\n            \n            visualize_seismic_data(input_files)\n            analyze_velocity_models(output_files)\n            \n        elif choice == '7':\n            print(\"\\nGenerating submission...\")\n            # Quick data loading to get stats\n            train_loader, val_loader, stats = load_and_prepare_data(CONFIG)\n            \n            if CONFIG['approach'] == 'thresholding':\n                generate_threshold_submission(CONFIG['test_dir'], CONFIG['submission_path'], CONFIG, stats)\n            else:\n                model = load_best_model(CONFIG)\n                if model:\n                    generate_model_submission(model, CONFIG['test_dir'], CONFIG['submission_path'], CONFIG, stats)\n                else:\n                    print(\"No trained model found. Run training first or use thresholding approach.\")\n                    \n        elif choice == '8':\n            print(\"\\nComparing approaches...\")\n            update_config({'compare_approaches': True})\n            run_pipeline(CONFIG)\n            update_config({'compare_approaches': False})\n            \n        elif choice == '9':\n            print(\"Exiting experiment manager\")\n            break\n            \n        else:\n            print(\"Invalid choice. Please enter a number between 1 and 9.\")\n            \n    print(\"Thank you for using the Geophysical Experiment Manager\")\n\n\n# Run the experiment manager\nif __name__ == \"__main__\" and 'ipykernel' in sys.modules:\n    # Only run interactively in notebook environment\n    experiment_manager()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T22:12:57.854291Z","iopub.status.idle":"2025-04-30T22:12:57.854632Z","shell.execute_reply.started":"2025-04-30T22:12:57.854466Z","shell.execute_reply":"2025-04-30T22:12:57.854481Z"}},"outputs":[],"execution_count":null}]}